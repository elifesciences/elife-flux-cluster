defaultConfig:
  airflow:
    dagParameters:
      schedule: null
      max_active_runs: 1
      tags:
        - 'Kubernetes'
  image: '${data_hub_core_dags_stable_image_repo}:${data_hub_core_dags_stable_image_tag}'
  env:
    - name: DEPLOYMENT_ENV
      value: prod
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: /dag_secret_files/gcloud/credentials.json
    - name: AWS_CONFIG_FILE
      value: /dag_secret_files/aws/credentials
  volumeMounts:
    - name: gcloud-secret-volume
      mountPath: /dag_secret_files/gcloud/
      readOnly: true
    - name: aws-secret-volume
      mountPath: /dag_secret_files/aws
      readOnly: true
    - name: data-hub-config-volume
      mountPath: /dag_config_files/
      readOnly: true
  volumes:
    - name: aws-secret-volume
      secret:
        secretName: credentials
    - name: gcloud-secret-volume
      secret:
        secretName: gcloud
    - name: data-hub-config-volume
      configMap:
        name: data-hub-configs
  resources:
    limits:
      memory: 1Gi
      cpu: 1000m
    requests:
      memory: 1Gi
      cpu: 100m

kubernetesPipelines:

  - dataPipelineId: 'Elife_Article_Xml_Pipeline_Kubernetes'
    airflow:
      dagParameters:
        schedule: '0 3 * * *'  # At 03:00, every day
    arguments:
      - 'python'
      - '-m'
      - 'data_pipeline.elife_article_xml.cli'
    env:
      - name: ELIFE_ARTICLE_XML_CONFIG_FILE_PATH
        value: /dag_config_files/elife-article-xml.config.yaml
      - name: GITHUB_API_AUTHORIZATION_FILE_PATH
        value: /dag_secret_files/github_api/github_api_authorization.txt
    volumeMounts:
      - name: github-api-secret-volume
        mountPath: /dag_secret_files/github_api/
        readOnly: true
    volumes:
      - name: github-api-secret-volume
        secret:
          secretName: github-api

  - dataPipelineId: 'BigQuery_To_OpenSearch_Pipeline_Kubernetes'
    airflow:
      dagParameters:
        schedule: "0 13 * * *" # At 13:00, every day
    arguments:
      - 'python'
      - '-m'
      - 'data_pipeline.opensearch.cli'
    env:
      - name: BIGQUERY_TO_OPENSEARCH_CONFIG_FILE_PATH
        value: /dag_config_files/bigquery-to-opensearch--prod.yaml
      - name: OPENSEARCH_USERNAME_FILE_PATH
        value: /dag_secret_files/opensearch/username
      - name: OPENSEARCH_PASSWORD_FILE_PATH
        value: /dag_secret_files/opensearch/password
    volumeMounts:
      - name: opensearch-secret-volume
        mountPath: /dag_secret_files/opensearch/
        readOnly: true
    volumes:
      - name: opensearch-secret-volume
        secret:
          secretName: opensearch-prod-admin-password

  - dataPipelineId: 'EuropePmc_Pipeline_Kubernetes'
    airflow:
      dagParameters:
        schedule: '0 6 * * *'  # At 06:00, every day
    arguments:
      - 'python'
      - '-m'
      - 'data_pipeline.europepmc.cli_europepmc'
    env:
      - name: EUROPEPMC_CONFIG_FILE_PATH
        value: /dag_config_files/europepmc.config.yaml

  - dataPipelineId: 'EuropePmc_Labslink_Pipeline_Kubernetes'
    airflow:
      dagParameters:
        schedule: '40 * * * *' # 40 past the hour, every day
    arguments:
      - 'python'
      - '-m'
      - 'data_pipeline.europepmc.cli_europepmc_labslink'
    env:
      - name: EUROPEPMC_LABSLINK_CONFIG_FILE_PATH
        value: /dag_config_files/europepmc-labslink--prod.config.yaml
      - name: EUROPEPMC_LABSLINK_FTP_PASSWORD_FILE_PATH
        value: /dag_secret_files/europepmc_labslink_ftp_credentials/password
      - name: EUROPEPMC_LABSLINK_FTP_DIRECTORY_NAME_FILE_PATH
        value: /dag_secret_files/europepmc_labslink_ftp_credentials/directory_name
    volumeMounts:
      - name: europepmc-labslink-ftp-credentials-volume
        mountPath: /dag_secret_files/europepmc_labslink_ftp_credentials/
        readOnly: true
    volumes:
      - name: europepmc-labslink-ftp-credentials-volume
        secret:
          secretName: europepmc-labslink-ftp-credentials--prod

  - dataPipelineId: 'Materialize_BigQuery_Views_Pipeline_Kubernetes'
    airflow:
      dagParameters:
        schedule: '15,45 3-21 * * 1-5' # At minute 15 and 45 past every hour from 3 through 21 on every day-of-week from Monday through Friday.
    arguments:
      - 'python'
      - '-m'
      - 'data_pipeline.bigquery_views.cli'
    env:
      - name: MATERIALIZE_BIGQUERY_VIEWS_CONFIG_PATH
        value: "s3://prod-elife-data-pipeline/airflow-config/bigquery-views"
      - name: MATERIALIZE_BIGQUERY_VIEWS_GCP_PROJECT
        value: "elife-data-pipeline"
      - name: MATERIALIZE_BIGQUERY_VIEWS_DATASET
        value: prod
