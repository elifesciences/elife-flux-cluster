global:
  resolve_timeout: 5m

route:
  receiver: 'slack'
  group_by: ['alertname', 'namespace', 'job']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  routes:
    - matchers:
      - "alertname = Watchdog"
      group_interval: 1m
      repeat_interval: 1m
      receiver: 'healthchecks.io'
    - matchers:
      - "alertname = Watchdog"
      receiver: "Watchdog"
    - matchers:
      - "alertname = InfoInhibitor"
      receiver: 'null'

receivers:
- name: 'null'
- name: 'slack'
  slack_configs:
  - api_url: '<SLACK_WEBHOOK_URL>'
    channel: 'cluster-alerts'
    send_resolved: true
    color: '{{ if eq .Status "resolved" }}good{{ else if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}'
    title: '{{ .Status }}: {{ .GroupLabels.alertname }}'
    title_link: 'https://alertmanager.elifesciences.org'
    text: >-
      {{ .CommonAnnotations.message }}
      -<https://prometheus.elifesciences.org/alerts|:fire:Prometheus>
      -<https://grafana.elifesciences.org|:chart_with_upwards_trend:Grafana>
      -<https://k8s-dashboard.elifesciences.org|:wheel_of_dharma:k8s Dashboard>
      {{ if ne .CommonAnnotations.runbook_url "" }}-<{{ .CommonAnnotations.runbook_url }}|:book:Runbook>{{ end -}}
- name: 'healthchecks.io'
  webhook_configs:
  - url: '<HEALTHCHECKS_WEBHOOK_URL>'
    send_resolved: false
- "name": "Watchdog"
